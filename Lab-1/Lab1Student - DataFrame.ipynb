{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Apache Spark lab, part 1: Basic concepts\n",
    "This notebook guides you through the basic concepts to start working with Apache Spark, including how to set up your environment, create and analyze data sets, and work with data files.\n",
    "\n",
    "This notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs on Python 2 with Spark 2.X.\n",
    "\n",
    "If you are new to notebooks, here's how the user interface works: [Parts of a notebook](http://datascience.ibm.com/docs/content/analyze-data/parts-of-a-notebook.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Apache Spark\n",
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for processing structured data, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "\n",
    "A Spark program has a driver program and worker programs. Worker programs run on cluster nodes or in local threads. Data sets are distributed\u001d",
    " across workers. \n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "In the first four sections of this notebook, you'll learn about Spark with very simple examples. In the last two sections, \n",
    "you'll use what you learned to analyze data files that have more realistic data sets.\n",
    "\n",
    "1. [Work with the SparkSession](#sparkcontext)<br>\n",
    "    1.1 [Invoke the SparkSession and get the version](#sparkcontext1)<br>\n",
    "2. [Working with DataFrames](#rdd)<br>\n",
    "    2.1 [Create a DataFrame](#rdd1)<br>\n",
    "    2.2 [View the data](#rdd2)<br>\n",
    "    2.3 [Create another DataFrame](#rdd3)<br>\n",
    "3. [Manipulate data in DataFrames](#trans)<br>\n",
    "    3.1 [Update numeric values](#trans1)<br>\n",
    "    3.2 [Add numbers in an array](#trans2)<br>\n",
    "    3.3 [Split and count strings](#trans3)<br>\n",
    "    3.4 [Count words](#trans4)<br>\n",
    "    3.5 [Explore a new function](#trans5)<br>\n",
    "    3.6 [Use groupBy and count()](#trans6)<br>\n",
    "4. [Filter data](#filter)<br>\n",
    "5. [Analyze text data from a file](#wordfile)<br>\n",
    "    5.1 [Get the data from a URL](#wordfile1)<br>\n",
    "    5.2 [Create a DataFrame from the file](#wordfile2)<br>\n",
    "    5.3 [Filter for a word](#wordfile3)<br>\n",
    "    5.4 [Count instances of a string at the beginning of words](#wordfile4)<br>\n",
    "    5.5 [Count instances of a string within words](#wordfile5)<br>\n",
    "6. [Analyze numeric data from a file](#numfile)<br>\n",
    "7. [Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Hello Spark\n",
    "\n",
    "This lab will introduce you to Apache Spark.  It is written in Python and runs in IBM's Data Science Experience environment through a Jupyter notebook.  While you work, it will be valuable to reference the [Apache Spark Documentation](http://spark.apache.org/docs/latest/programming-guide.html).  Since it is Python, be careful of whitespace!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkcontext\"></a>\n",
    "## Step 1 - Working with the SparkSession object\n",
    "\n",
    "The Apache Spark driver application uses the SparkSession object to allow a programming interface to interact with the driver application. The SparkSession object tells Spark how and where to access a cluster.\n",
    "\n",
    "The Data Science Experience notebook environment predefines the SparkSession for you.   This context variable will always be called 'spark'.\n",
    "\n",
    "\n",
    "<a id=\"sparkcontext1\"></a>\n",
    "### Step 1.1 - Using the spark session object, reading the <i>version</i> attribute will return the working version of Apache Spark<br><br>\n",
    " <div class=\"panel-group\" id=\"accordion-11\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse1-11\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-11\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">The spark session is automatically set in a Jupyter notebook.   It is called: spark</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse2-11\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-11\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>spark.version</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse3-11\">\n",
    "        Optional</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-11\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Jupyter notebooks have command completion which can be invoked via the TAB key.<br>Type:<br>&nbsp;&nbsp;&nbsp;&nbsp;<i>spark.&lt;TAB&gt;</i><br>to see all the possible options within the Spark session</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 - Check spark version\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd\"></a>\n",
    "## Step 2 - Working with DataFrames\n",
    "\n",
    "Apache Spark uses an abstraction for working with data called a DataFrame. A DataFrame is a structured collection of elements with a schema that can be operated on in parallel. DataFrames are immutable, so you can't update the data in them. To update data in a DataFrame, you must create a new DataFrame. In Apache Spark, work is done by creating new DataFrames, transforming existing DataFrames, or using DataFrame to compute results. When working with DataFrames, the Spark driver application automatically distributes the work across the cluster.\n",
    "\n",
    "You can construct DataFrames by parallelizing existing Python collections (lists), by manipulating DataFrames, or by manipulating files in HDFS or any other storage system.\n",
    "\n",
    "You can run these types of methods on DataFrames: \n",
    " - Actions: query the data and return values\n",
    " - Transformations: manipulate data values and return pointers to new DataFrames. \n",
    "\n",
    "Find more information on Python methods in the [PySpark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html).\n",
    "\n",
    "<a id=\"rdd1\"></a>\n",
    "### Step 2.1 - Create a DataFrame with numbers 1 to 10\n",
    "\n",
    "There are several ways to create DataFrame: parallelizing an existing collection, referencing a dataset in an external storage system which offers a Hadoop InputFormat -- or transforming an existing DataFrame.<br>\n",
    "<br>\n",
    "Create an iterable or collection in your program with numbers 1 to 10 as a DataFrame. Invoke the Spark Session's (spark) <i>range()</i> method on it.<br>\n",
    "\n",
    " <div class=\"panel-group\" id=\"accordion-21\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse1-21\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-21\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "Use range(10) method on SparkSession object<br><br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse2-21\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-21\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "df = spark.range(10)<br>\n",
    "          You can use the show() method to display the DataFrame (e.g. df.show())<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    " </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame with numbers 1 to 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"rdd2\"></a>\n",
    "### Step 2.2 - View the data\n",
    "Return the first row in the DataFrame<br/><br/>\n",
    "<div class=\"panel-group\" id=\"accordion-22\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-22\" href=\"#collapse1-22\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-22\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Use the <i>first()</i> method on the DataFrame to return the first row in a DataFrame.   You could also use the <i>take()</i> method with a parameter of 1.   first() and take(1) are equivalent.   Both will take the first row in the DataFrame's 0th partition.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-22\" href=\"#collapse2-22\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-22\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type: <br/>\n",
    "df.first()</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the first row in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number is placed in a different row in the DataFrame. Because the first() method returned a value, it is an action.\n",
    "\n",
    "### Step 2.3 - Return an array of the first five elements<br><br>\n",
    " <div class=\"panel-group\" id=\"accordion-23\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-23\" href=\"#collapse1-23\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-23\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Use the <i>take()</i> method</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-23\" href=\"#collapse2-23\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-23\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "df.take(5)</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2.3 - Return an array of the first five elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd3\"></a>\n",
    "### 2.4 Create another DataFrame \n",
    "Create a DataFrame that contains multiple strings and print the value of the first string:\n",
    "\n",
    "<div class=\"panel-group\" id=\"accordion-26\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-26\" href=\"#collapse1-26\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-26\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Create a variable with the Strings \"Hello Human\" and \"My Name is Spark\". Create as a list of Python tuples (need to add a \",\" after the string elements). Then use the  createDataFrame method from the SparkSession object with the variable as the parameter. You can also provide a schema as a list of column names. Note, in this case there is only 1 column</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-26\" href=\"#collapse2-26\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-26\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "y = [(\"Hello Human\",),(\"My Name is Spark\",)]<br>\n",
    "df_y = spark.createDataFrame(y,['text'])<br>\n",
    "df_y.take(1)<br></div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-26\" href=\"#collapse3-26\">\n",
    "        Optional Advanced</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-26\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Is there a way to get the third element directly?</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2.4.1 create a string array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the collection into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2.4.2 put the collection into a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the first element in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2.4.3 view the first element\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You created the strings \"Hello Human\" and \"My Name is Spark\" and you returned \"Hello Human\" as the first row of the DataFrame. To analyze a set of words, you can \"explode\" each word into a DataFrame element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans\"></a>\n",
    "## 3. Manipulate data in DataFrames\n",
    "\n",
    "Remember that to manipulate data, you use transformation functions. Need to update this section (BLB)\n",
    "\n",
    "Here are some common Spark transformation functions that you'll be using in this notebook:\n",
    "\n",
    " - `map(func)`: returns a new RDD with the results of running the specified function on each element  \n",
    " - \n",
    "\n",
    "<a id=\"trans1\"></a>\n",
    "### 3.1 Update numeric values\n",
    "Increment each number in the DataFrame by 1. \n",
    "<br/>\n",
    " <div class=\"panel-group\" id=\"accordion-24\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-24\" href=\"#collapse1-24\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-24\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Use the <i>select</i> function on the DataFrame, and increment the value in the column by 1. To obtain the value in the column use df.id Use the show() function to display the results. Note, you can use the columns attribute on a DataFrame to get the list of columns in the DataFrame <br>\n",
    "</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-24\" href=\"#collapse2-24\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-24\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    " df.select(df.id+1).show()</div>\n",
    "    </div>\n",
    "  </div>\n",
    " </div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1 - Write the select function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans2\"></a>\n",
    "### 3.2 Sum the numbers in a DataFrame\n",
    "Calculate the sum of the numbers in the DataFrame df\n",
    "\n",
    "<div class=\"panel-group\" id=\"accordion-21\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion2-32\" href=\"#collapse3-32\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-32\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Import the sum function from pyspark.sql.functions. Use the DataFrame select function with the argument sum(df.id) or sum(\"id\"). Use the DataFrame show() function to display the result. You can also use the column alias function to name the \"sum\" column in the resulting DataFrame. \n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion2-32\" href=\"#collapse4-32\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse4-32\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "          from pyspark.sql.functions import sum <br>\n",
    " df.select(sum(\"id\")).show()  or df.select(sum(df.id)).show() <br><br>\n",
    "          Using the alias function <br>\n",
    "          df.select(sum(\"id\").alias(\"sum\")).show()\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3.2 Calculate the sum of the numbers in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans3\"></a>\n",
    "### 3.3 Split and count text strings\n",
    "\n",
    "Create a DataFrame with the following text strings and show the first element:<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\"IBM Data Science Experience is built for enterprise-scale deployment.\"<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\"Manage your data, your analytical assets, and your projects in a secured cloud environment.\"<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\"When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.\"<br/><br/>\n",
    " <div class=\"panel-group\" id=\"accordion-27\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-27\" href=\"#collapse1-27\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-27\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Recall you will need to create a list of tuples as input to the createDataFrame operation. The list of tuples is assigned as follows<br>\n",
    "          text = [(\"String1\",),(\"String2\",),(\"String3\",)]\n",
    "        </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-27\" href=\"#collapse2-27\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-27\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">text = [(\"IBM Data Science Experience is built for enterprise-scale deployment.\",),(\"Manage your data, your analytical assets, and your projects in a secured cloud environment.\",),(\"When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.\",)]<br/>\n",
    "text_df = spark.createDataFrame(text,['text'])<br>\n",
    "text_df.first()<br>  \n",
    "      </div>\n",
    "    </div>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3.3 create a DataFrame with the text strings. Recall that you need to create a list of tuples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans4\"></a>\n",
    "### Step 3.4 - Split all the entries in the DataFrame on the spaces.  Then print it out.  Pay careful attention to the new format.\n",
    "<br/>\n",
    " <div class=\"panel-group\" id=\"accordion-210\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-210\" href=\"#collapse1-210\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-210\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">To split on spaces, use the pyspark.sql.function <a href=\"\"><i>split()</i></a> function on the column in the DataFrame. You will need to import the function</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-210\" href=\"#collapse2-210\">\n",
    "        Hint 2</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-210\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Use the select function on the DataFrame to apply the split function to all the values in the text column. </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-210\" href=\"#collapse3-210\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-210\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type: <br>\n",
    "          from pyspark.sql.functions import split <br>\n",
    "          text_split_df = text_df.select(split(\"text\",\" \").alias(\"split_text\")) <br>\n",
    "          text_split_df.show() <br> </div>          \n",
    "     </div>\n",
    "    </div>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 3.4 Split entries in the DataFrame on spaces. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans5\"></a>\n",
    "### Step 3.5 - Explore a new function: <a href=\"\">explode</a>\n",
    "<br/>\n",
    "We want to count the words in <b>all</b> the lines. Currently each row in the DataFrame is now an array of words. We need to 'expode' the array so that each separate word is in its own row.  <br>\n",
    " <div class=\"panel-group\" id=\"accordion-211\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-211\" href=\"#collapse1-211\">\n",
    "        Hint </a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-211\" class=\"panel-collapse collapse\">\n",
    "        <div class=\"panel-body\">Import the <i>explode</i> function from pyspark.sql.functions. Apply the explode function to the column in the DataFrame </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-211\" href=\"#collapse2-211\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-211\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br/>\n",
    "       from pyspark.sql.functions import explode <br>\n",
    "       text_word_df = text_split_df.select(explode(\"split_text\").alias(\"word\"))<br>\n",
    "       print \"exploded words\" <br>\n",
    "       text_word_df.show() </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-211\" href=\"#collapse3-211\">\n",
    "        Optional Advanced</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-211\" class=\"panel-collapse collapse\">\n",
    "        <div class=\"panel-body\">Import the <i>lower</i> and <i>regexp_replace</i> functions from pyspark.sql.functions. The <i>lower</i> will transform the words to lowercase. The <i>regexp_replace</i> will allow you to replace the \",\" and \".\" with null character.  The regular expression to use is \"[\\.,]\" <br>\n",
    "            from pyspark.sql.functions import lower,regexp_replace <br>\n",
    "            print \"lower case\" <br>\n",
    "            text_word_lower_df = text_word_df.select(lower(text_word_df.word).alias(\"word\")) <br>\n",
    "            text_word_lower_df.show() <br>\n",
    "            print \"remove '.' and ','\" <br>\n",
    "            text_word_lower_df.select(regexp_replace(expr(\"word\"),\"[\\\\.,]\",\"\")).show()<br></div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 3.5 Explore the explode function\n",
    "\n",
    "\n",
    "#Optional Advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans6\"></a>\n",
    "### Step 3.6 - Use the groupBy  and count() function to count the number times each word appears.  \n",
    "<br>\n",
    " <div class=\"panel-group\" id=\"accordion-212\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-212\" href=\"#collapse1-212\">\n",
    "        Hint </a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-212\" class=\"panel-collapse collapse\">\n",
    "        <div class=\"panel-body\">Use the <i>groupBy</i> DataFrame operation and then use the count() operation. \n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-212\" href=\"#collapse3-212\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-212\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "    sum_df = text_word_df.groupBy(\"word\").count()<br>\n",
    "    sum_df.show()</div>\n",
    " \n",
    "   \n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3.6 - Use the groupBy and count() functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filter\"></a>\n",
    "## 4. Filter data\n",
    "\n",
    "The <i>where</i> DataFrame operation creates a new DataFrame fron another DataFrame based on a filter criteria expressed as a boolean.  \n",
    "\n",
    "\n",
    "Find the number of instances of the word `IBM` in the `sum_df` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4\n",
    "sum_df.select(\"word\",\"count\").where(insert code).show() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile\"></a>\n",
    "## 5. Analyze text data from a file\n",
    "In this section, you'll download a file from a URL, create a Data from it, and analyze the text in it.\n",
    "\n",
    "<a id=\"wordfile1\"></a>\n",
    "### Step 5.1 - Read the Apache Spark README.md file from Github.  The ! allows you to embed file system commands\n",
    "<br/>\n",
    "We remove README.md in case there was an updated version -- but also for another reason you will discover in Lab 2<br/><br/>\n",
    "Type:<br/>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;!rm README.md* -f<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;!wget https://raw.githubusercontent.com/apache/spark/master/README.md<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 5.1 - Pull data file into workbench\n",
    "!rm README.md* -f\n",
    "!wget https://raw.githubusercontent.com/apache/spark/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile1\"></a>\n",
    "### Step 5.2 - Create a DataFrame by reading from the local filesystem and count the number of lines  Here is the [textfile()](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile) documentation.<br><br>\n",
    " <div class=\"panel-group\" id=\"accordion-52\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-52\" href=\"#collapse1-52\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-52\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">README.md has been loaded into local storage so there is no path needed.   <i>text()</i> returns a DataFrame.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-52\" href=\"#collapse2-52\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-52\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "          lines = spark.read.text(\"README.md\")<br>\n",
    "          lines.count()<br></div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-52\" href=\"#collapse3-52\">\n",
    "        Optional Advanced</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-52\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">By default, <i>text()</i> uses UTF-8 format.   Read the file as UNICODE (refer to the docs).</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2 - Create DataFrame from data file\n",
    "\n",
    "\n",
    "# Optional Advanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile3\"></a>\n",
    "### Step 5.3 - Use \"where\" to filter lines that contain \"Spark\". <br>\n",
    "We will also take a look at the first line in the newly filtered RDD. <br><br>\n",
    " <div class=\"panel-group\" id=\"accordion-33\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse1-33\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-33\" class=\"panel-collapse collapse\">\n",
    "        <div class=\"panel-body\">use the <i>where</i> operation with the <i>like</i> operation</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse2-33\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-33\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>          \n",
    "      Spark_lines = lines.where(lines.value.like(\"%Spark%\"))<br>\n",
    "      Spark_lines.first()</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse3-33\">\n",
    "        Advanced Optional</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-33\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">There are 20 lines which contain the word \"Spark\".   Find all lines which contain it when case-insensitive<br></div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5.3 - Filter for only lines with word Spark\n",
    "\n",
    "\n",
    "# Advanced optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile4\"></a>\n",
    "### Step 5.4 - Print the number of Spark lines in this filtered DataFrame out of the total number and print the result as a concatenated string.<br/><br/>\n",
    " <div class=\"panel-group\" id=\"accordion-34\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-34\" href=\"#collapse1-34\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-34\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">The <i>print()</i> statement prints to the console.  (Note: be careful on a cluster because a print on a distributed machine will not be seen).  You can cast integers to string by using the <i>str()</i> method.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-34\" href=\"#collapse2-34\">\n",
    "        Hint 2</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-34\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Strings can be concatenated together with the + sign.   You can mark a statement as spanning multiple lines by putting a \\ at the end of the line.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-34\" href=\"#collapse3-34\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-34\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type:<br>\n",
    "      print \"The file README.md has \" + str(Spark_lines.count()) +\n",
    "      \" of \" + str(lines.count()) + \n",
    "      \" lines with the word Spark in it.\"          \n",
    "</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.4 - count the number of lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile5\"></a>\n",
    "### Step 5.5 - Now count the number of times the word Spark appears in the original text, not just the number of lines that contain it.\n",
    " <div class=\"panel-group\" id=\"accordion-35\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-35\" href=\"#collapse1-35\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-35\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "        Looking back at previous exercises, you will need to: <br>\n",
    "          &nbsp;&nbsp;&nbsp;&nbsp;1 - Use the <i>explode</i> function on the original DataFrame and split on white space.<br>\n",
    "          &nbsp;&nbsp;&nbsp;&nbsp;2 - Use the <i>where</i> operation with the <i>like</i> operation to include all instances of Spark. Note Spark will appear in several \\\"words\\\"<br>\n",
    "          &nbsp;&nbsp;&nbsp;&nbsp;3 - Use the <i>count</i> function to count all instances of each word. <br>\n",
    "          &nbsp;&nbsp;&nbsp;&nbsp;4 - Print the total count<br><br>        \n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-35\" href=\"#collapse2-35\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-35\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "      words = lines.select(explode(split(lines.value,\" \")).alias(\"value\"))<br>\n",
    "      Spark_count = words.where(words.value.like(\"%Spark%\")).count()<br>\n",
    "      print(\"Number of Spark Mentions:\"+ str(Spark_count))<br>\n",
    "      </div>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-35\" href=\"#collapse3-35\">\n",
    "        Optional Advanced</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-35\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Put the entire statement on one line and make the filter case-insensitive.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.5\n",
    "\n",
    "\n",
    "#Optional Advanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"numfile\"></a>\n",
    "## Step 6 - Perform analysis on a data file\n",
    "This part is a little more open ended and there are a few ways to complete it.  Scroll up to previous examples for some guidance.  You will download a data file, transform the data, and then average the prices.  The data file will be a sample of tech stock prices over six days. <br>\n",
    "\n",
    "Data Location: https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv<br>\n",
    "The data file is a csv<br/><br/>\n",
    "Here is a sample of the file:<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IBM,159.720001,159.399994,158.880005,159.539993,159.550003,160.350006\n",
    "\n",
    "We leverage map-reduce to create a generic solution but there are multiple ways to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 6 - Delete the file if it exists, download a new copy and load it into an RDD\n",
    "!rm StockPrices.csv -f\n",
    "!wget https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv\n",
    "    \n",
    "SP = spark.read.text(\"StockPrices.csv\")\n",
    "SP.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = spark.read.text(\"StockPrices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_split = stocks.select(split(\"value\",\",\").alias(\"value\"))\n",
    "stocks_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_grid=stocks_split.select(stocks_split.value[0],stocks_split.value[1],stocks_split.value[2],stocks_split.value[3],stocks_split.value[4],stocks_split.value[5])\n",
    "stock_grid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 7. Summary and next steps\n",
    "\n",
    "You've learned how to work with data in RDDs to discover useful information.\n",
    "\n",
    "Dig deeper:\n",
    " - [Apache Spark documentation](http://spark.apache.org/documentation.html)\n",
    " - [PySpark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "Carlo Appugliese is a Spark and Hadoop evangelist at IBM.<br/>\n",
    "Braden Callahan is a Big Data Technical Specialist for IBM.<br/>\n",
    "Ross Lewis is a Big Data Technical Sales Specialist for IBM.<br/>\n",
    "Mokhtar Kandil is a World Wide Big Data Technical Specialist for IBM.<br/>\n",
    "Joel Patterson is a Big Data Technical Specialist for IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
